(this["webpackJsonpkdd-workshop"]=this["webpackJsonpkdd-workshop"]||[]).push([[0],{101:function(e,t,n){"use strict";n.r(t);var s=n(0),i=n.n(s),r=n(18),a=n.n(r),c=(n(88),n(89),n(90),n(32)),o=n(9),l=n(105),d=n(1),h={"/":"1","/organizers":"2","/timeline":"3","/submission":"4","/speakers":"5"};var j=function(){var e=Object(o.f)();return Object(d.jsxs)(l.a,{className:"navigator",mode:"horizontal",selectedKeys:h[e.pathname],children:[Object(d.jsxs)(l.a.Item,{children:["   ",Object(d.jsx)(c.b,{to:"/",children:"HOME"}),"                 "]},"1"),Object(d.jsxs)(l.a.Item,{children:["   ",Object(d.jsx)(c.b,{to:"/organizers",children:"ORGANIZERS"})," "]},"2"),Object(d.jsxs)(l.a.Item,{children:["   ",Object(d.jsx)(c.b,{to:"/timeline",children:"TIMELINE"}),"     "]},"3"),Object(d.jsxs)(l.a.Item,{children:["   ",Object(d.jsx)(c.b,{to:"/submission",children:"CALL FOR PAPERS"}),"     "]},"4"),Object(d.jsxs)(l.a.Item,{children:["   ",Object(d.jsx)(c.b,{to:"/speakers",children:"SPEAKERS"}),"     "]},"5")]})};var m=function(){return Object(d.jsx)("div",{children:Object(d.jsx)("div",{className:"container",children:Object(d.jsx)("div",{className:"row justify-content-center",children:Object(d.jsxs)("div",{className:"col-lg-8 cl-md-10",children:[Object(d.jsx)("h2",{children:"Introduction"}),Object(d.jsx)("p",{className:"contentText",children:"Recent years have witnessed the prosperity of deep learning in a variety of applications, such as computer vision, natural language processing and speech recognition and enhancement. And this success stimulates the proliferation of many benchmark deep neural models. Before practitioners develop models for the task of interest, they can actually have access to some off-the-shelf models, which are already trained on the same, similar or even totally different tasks. Those pretrained models are usually dedicated to mine the training data well, and the informative knowledge is supposed to exist in the learned representations or the model weights themselves. In this sense, how to mine the knowledge in the pretrained models is also of significance in achieving more promising performance."}),Object(d.jsx)("p",{className:"contentText",children:"There are multiple ways to fulfill the model mining. For example, inspired by the human education, we can leverage the pretrained models as privileged information or additional resources following the teacher-student paradigm. Then the knowledge can be manifested and distilled on the learned logits or intermediate features, such as attention maps, similarity among examples, and local graphs. As a result, the student models are expected to perform better with the help of the mined knowledge in teacher models. Likewise, pretrained models can also serve as a general yet powerful platform for adapting customized models for different target tasks. For example, large-scale pretrained models with huge parameters as BERT, GPT-3, and ViT show great generalization ability in many downstream tasks, and we can have decent performance for the target task with only limited data."}),Object(d.jsx)("div",{style:{height:20}}),Object(d.jsx)("h2",{children:"Topics of Interest"}),Object(d.jsx)("p",{className:"contentText",children:"The TOPIC model mining of this workshop aims to investigate more diverse and advanced manners in mining knowledge within models, which tends to leverage the pretrained models more wisely, elegantly and systematically. The topics of interest include the theory, algorithm and application of mining the values of models in different scenarios"}),Object(d.jsx)("div",{children:Object(d.jsxs)("ul",{className:"contentText",children:[Object(d.jsx)("li",{children:"Distilling a lightweight model from a well-trained heavy model, e.g., knowledge distillation or the teacher-student paradigm;"}),Object(d.jsx)("li",{children:"Continuously upgrading the model with new data, new concept or new tasks, e.g., continual learning, lifelong learning, zero-shot learning and multi-task learning."}),Object(d.jsx)("li",{children:"Boosting the performance of the model by carefully designing the predecessor tasks, e.g., pre-training, self-supervised and contrastive learning."}),Object(d.jsx)("li",{children:"Transferring the models across different datasets, domains or modalities, e.g., transfer learning and domain adaptation."})]})}),Object(d.jsx)("p",{className:"contentText",children:"Model mining as a special way of data mining is relevant to SIGKDD, and its audience including researchers and engineers will benefit a lot for designing more advanced algorithms for their tasks."})]})})})})};var b=function(){return Object(d.jsx)("div",{style:{paddingBottom:400},children:Object(d.jsx)("div",{className:"container",children:Object(d.jsx)("div",{className:"row justify-content-center",children:Object(d.jsxs)("div",{className:"col-lg-8 cl-md-10",children:[Object(d.jsx)("h2",{children:"Workshop Timeline"}),Object(d.jsx)("table",{className:"table",children:Object(d.jsxs)("tbody",{children:[Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Workshop Website and CFP"}),Object(d.jsx)("td",{children:"April 10th, 2021"})]}),Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Workshop Paper Submission"}),Object(d.jsx)("td",{children:"May 20th, 2021"})]}),Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Workshop Paper Notification"}),Object(d.jsx)("td",{children:"June 10th, 2021"})]}),Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Updates to Workshop Chairs (number of papers, acceptance rate etc.)"}),Object(d.jsx)("td",{children:"June 20th, 2021"})]}),Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Workshop Program Final Submission and Full Website Online"}),Object(d.jsx)("td",{children:"July 2nd, 2021"})]}),Object(d.jsxs)("tr",{children:[Object(d.jsx)("td",{children:"Workshop Dates"}),Object(d.jsx)("td",{children:"August 14-18th, 2021"})]})]})})]})})})})};var p=function(){return Object(d.jsx)("div",{style:{paddingBottom:100},children:Object(d.jsx)("div",{className:"container",children:Object(d.jsx)("div",{className:"row justify-content-center",children:Object(d.jsxs)("div",{className:"col-lg-8 cl-md-10",children:[Object(d.jsx)("h2",{children:"Organizers"}),Object(d.jsx)("div",{children:Object(d.jsxs)("p",{className:"organizersText",children:[Object(d.jsxs)("a",{href:"https://shanyou92.github.io/",children:[" ",Object(d.jsx)("b",{children:"Shan You"})," "]})," is currently a Researcher at SenseTime, and also a post doc at Tsinghua University. Before that, he received a Bachelor of mathematics and applied mathematics (elite class) from Xi'an Jiaotong University, and a Ph.D. degree of computer science from Peking University. During pursing his PhD degree, he visited the UBTech Sydney AI Institute, the University of Sydney, and received awards from Qualcomm. His research interests include AutoML, reinforcement learning and other computer vision and machine learning topics, such as face recognition and object tracking. He has published his research outcomes in many top tier conferences and transactions."]})}),Object(d.jsx)("div",{children:Object(d.jsxs)("p",{className:"organizersText",children:[Object(d.jsxs)("a",{href:"http://changxu.xyz/",children:[" ",Object(d.jsx)("b",{children:" Chang Xu "})," "]}),"  is a Senior Lecturer and ARC DECRA Fellow at the School of Computer Science, University of Sydney. His recent interests focus on efficient deep learning and sparse networks, robust learning under noisy data or labels, and neural architecture search. He has published over 100 papers in prestigious journals and top tier conferences. He has received several paper awards, including Distinguished Paper Award in IJCAI 2018. He regularly severed as the senior PC or PC for many conferences, e.g., ICLR, SIGKDD, NIPS, ICML, CVPR, ICCV, IJCAI and AAAI. He has been recognized as Top Ten Distinguished Senior PC Member in IJCAI 2017."]})}),Object(d.jsx)("div",{children:Object(d.jsxs)("p",{className:"organizersText",children:[Object(d.jsxs)("a",{href:"http://wangfei.info/",children:[" ",Object(d.jsx)("b",{children:"Fei Wang"})," "]}),"  is the Associate Research Director of SenseTime. He leads a vibrant team to do fundamental research of computer vision and develop comprehensive solutions for the mobile intelligence industry. The goal of his team is to establish AI systems that make edge devices more intelligent and efficient. His research interests include Deep Learning, Edge Computation, etc. He has gained over 1000 Google Scholar Citations with recent publications during the last few years. Fei obtained his Bachelor\u2019s degree and Master's degree from Beijing University of Posts and Telecommunications."]})}),Object(d.jsx)("div",{children:Object(d.jsxs)("p",{className:"organizersText",children:[Object(d.jsxs)("a",{href:"http://bigeye.au.tsinghua.edu.cn/english/Introduction.html",children:[" ",Object(d.jsx)("b",{children:"Changshui Zhang"})," "]}),"  received the B.E. degree in mathematics from Peking University, Beijing, China, in 1986, and the M.S. and Ph.D. degrees in control science and engineering from Tsinghua University, Beijing, in 1989 and 1992, respectively. In 1992, he joined the Department of Automation, Tsinghua University, where he is currently a professor. His research interests include pattern recognition and machine learning. He is a Fellow member of the IEEE."]})})]})})})})};var g=function(){return Object(d.jsx)("div",{children:Object(d.jsx)("div",{className:"container",children:Object(d.jsx)("div",{className:"row justify-content-center",children:Object(d.jsxs)("div",{className:"col-lg-8 cl-md-10",children:[Object(d.jsx)("h2",{children:"Speakers"}),Object(d.jsx)("h4",{children:"TBA"})]})})})})};var u=function(){return Object(d.jsx)("div",{children:Object(d.jsx)("div",{className:"container",children:Object(d.jsx)("div",{className:"row justify-content-center",children:Object(d.jsxs)("div",{className:"col-lg-8 cl-md-10",children:[Object(d.jsx)("h2",{children:"CALL FOR PAPERS"}),Object(d.jsxs)("p",{children:["Submissions should follow the "," ",Object(d.jsx)("a",{href:"https://kdd.org/kdd2021/calls/view/call-for-research-track-papers",children:Object(d.jsx)("b",{children:"SIGKDD formatting requirements"})}),"   "," ","and will be evaluated using the "," ",Object(d.jsx)("a",{href:"https://kdd.org/kdd2021/calls/view/call-for-research-track-papers",children:Object(d.jsx)("b",{children:"SIGKDD Research Track evaluation criteria"})}),". "," ","Preference will be given to papers that are reproducible, and authors are encouraged to share their data and code publicly whenever possible. Submissions are strongly recommended to be ",Object(d.jsx)("b",{children:" no more than 4 pages "}),", excluding references or supplementary materials (all in a single pdf). The appropriateness of using additional pages over the recommended length will be judged by reviewers. Papers must be submitted in PDF format to easychair",Object(d.jsxs)("a",{href:"https://easychair.org/conferences/?conf=wmm2021",children:[" ",Object(d.jsx)("b",{children:" https://easychair.org/conferences/?conf=wmm2021 "})," "]}),"   "," ","and formatted according to the new",Object(d.jsxs)("a",{href:"https://www.acm.org/publications/proceedings-template",children:[" ",Object(d.jsx)("b",{children:" Standard ACM Conference Proceedings Template "})," "]})," ."]}),Object(d.jsx)("p",{children:"The review process is single-round and double-blind (submission files have to be anonymized). The program committee will select the papers based on originality, presentation, and technical quality for spotlight and/or poster presentation. Concurrent submissions to other journals and conferences are acceptable."}),Object(d.jsxs)("p",{children:["All questions about submissions should be emailed to  ",Object(d.jsxs)("a",{href:"mailto: youshan@sensetime.com",children:["  ",Object(d.jsx)("b",{children:" youshan@sensetime.com "})," "]})]})]})})})})},f=n(103),x=f.a.Content;var O=function(){return Object(d.jsx)(c.a,{children:Object(d.jsxs)(f.a,{className:"layout",children:[Object(d.jsx)(j,{}),Object(d.jsxs)(x,{children:[Object(d.jsx)("div",{className:"imageOverlay",children:Object(d.jsxs)("h1",{style:{textAlign:"center",color:"white",position:"relative"},children:["KDD Workshop 2021 ",Object(d.jsx)("br",{}),"Model Mining"]})}),Object(d.jsx)("div",{style:{padding:"0 50px",marginTop:30,marginBottom:20,minHeight:500},children:Object(d.jsxs)(o.c,{children:[Object(d.jsx)(o.a,{path:"/organizers",component:p}),Object(d.jsx)(o.a,{path:"/timeline",component:b}),Object(d.jsx)(o.a,{path:"/speakers",component:g}),Object(d.jsx)(o.a,{path:"/submission",component:u}),Object(d.jsx)(o.a,{path:"/",component:m})]})})]})]})})},v=function(e){e&&e instanceof Function&&n.e(3).then(n.bind(null,106)).then((function(t){var n=t.getCLS,s=t.getFID,i=t.getFCP,r=t.getLCP,a=t.getTTFB;n(e),s(e),i(e),r(e),a(e)}))};a.a.render(Object(d.jsx)(i.a.StrictMode,{children:Object(d.jsx)(O,{})}),document.getElementById("root")),v()},88:function(e,t,n){},90:function(e,t,n){}},[[101,1,2]]]);
//# sourceMappingURL=main.043f4e18.chunk.js.map